{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from cell_model_full_parallel import Cell_Population\n",
    "from replaybuffer import ReplayBuffer\n",
    "from deepQLnetwork import Model\n",
    "from joblib import Parallel, delayed\n",
    "from utils_figure_plot import DynamicUpdate, plot_reward_Q_loss\n",
    "import copy\n",
    "from scipy import signal\n",
    "import matplotlib as mpl\n",
    "# import wandb\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "\n",
    "class CDQL:\n",
    "    def __init__(self, delay_embed_len=10,\n",
    "            batch_size=16,\n",
    "            delta_t=0.2,\n",
    "            omega=0.02,\n",
    "            gamma=0.99,\n",
    "            update_freq=2,\n",
    "            use_gpu=False,\n",
    "            num_cells_init=60,\n",
    "            kn0_mean=2.55,\n",
    "            T=12,\n",
    "            agent_input=\"full\"):\n",
    "\n",
    "        self.gamma = gamma\n",
    "        if use_gpu and torch.cuda.is_available(): # and torch.cuda.device_count() > 1:\n",
    "            self.device = torch.device('cuda')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "        assert (not use_gpu) or (self.device == torch.device('cuda'))\n",
    "\n",
    "        self.delta_t = delta_t\n",
    "        self.sim_controller = Cell_Population(num_cells_init, delta_t, omega, kn0_mean, T)\n",
    "        self.buffer = ReplayBuffer(1e6)\n",
    "        self.delay_embed_len = delay_embed_len\n",
    "        self.batch_size = batch_size\n",
    "        self.b_actions = [0.0, 3.72]\n",
    "        self.b_num_actions = len(self.b_actions)\n",
    "        self.num_actions = self.b_num_actions\n",
    "        self.b_index = [0, 1]\n",
    "\n",
    "        if agent_input == \"full\":\n",
    "            self.get_state_reward = self.sim_controller.get_reward_all\n",
    "            self.embed_multiplier = 3\n",
    "        elif agent_input == \"no_nutrient\":\n",
    "            self.get_state_reward = self.sim_controller.get_reward_no_nutrient\n",
    "            self.embed_multiplier = 2\n",
    "        elif agent_input == \"no_act\":\n",
    "            self.get_state_reward = self.sim_controller.get_reward_no_antibiotic\n",
    "            self.embed_multiplier = 2\n",
    "        else:\n",
    "            raise ValueError(\"Invalid agent_input value.\")\n",
    "\n",
    "        self.model = Model(self.device, num_inputs=self.delay_embed_len*self.embed_multiplier, num_actions=self.num_actions)\n",
    "\n",
    "        self.init = 0.0 # b_init\n",
    "        self.max_pop = 1e11 # threshold for terminating episode\n",
    "        self.loss = []\n",
    "        self.ave_sum_rewards = []\n",
    "        self.std_sum_rewards = []\n",
    "        self.ave_Q1 = []\n",
    "        self.ave_Q2 = []\n",
    "        self.ave_Q1_target = []\n",
    "        self.ave_Q2_target = []\n",
    "        self.grad_updates = []\n",
    "        self.training_iter = 0\n",
    "        self.update_freq = update_freq\n",
    "        self.epsilon = None\n",
    "        self.episode_num = 0\n",
    "\n",
    "    def _to_tensor(self, x):\n",
    "        return torch.tensor(x).float().to(self.device)\n",
    "\n",
    "    def _save_data(self, episode_num):\n",
    "        np.save(self.folder_name + \"/replaybuffer\", np.array(self.buffer.buffer, dtype=object))\n",
    "\n",
    "        self.model.save_networks(self.folder_name+'/')\n",
    "        self.save_episode_num(episode_num)\n",
    "\n",
    "    def save_episode_num(self, episode_num, filename='/episode_num.txt'):\n",
    "        filename = self.folder_name + filename\n",
    "        with open(filename,\"w\") as file:\n",
    "            file.write(str(episode_num))\n",
    "\n",
    "    def load_data(self, sweep_var, folder_name=\"./Results\"):\n",
    "        self.folder_name = folder_name + str(sweep_var)\n",
    "        self.buffer.load_buffer(self.folder_name + \"/replaybuffer.npy\")\n",
    "        self.model.load_networks(self.folder_name)\n",
    "        self.episode_num = self.load_episode_num()\n",
    "\n",
    "    def load_episode_num(self, filename='/episode_num.txt'):\n",
    "        filename = self.folder_name + filename\n",
    "        try:\n",
    "            with open(filename, \"r\") as file:\n",
    "                episode_num = int(file.read().strip()) + 1\n",
    "            print(\"Partially trained model found, starting from episode \",episode_num,\".\")\n",
    "            return episode_num\n",
    "        except FileNotFoundError:\n",
    "            return 0\n",
    "\n",
    "    def _get_action(self, state, episode=None, eval=False):\n",
    "        \"\"\"Gets action given some state using epsilon-greedy algorithm\n",
    "        Args:\n",
    "            state: List defining the state\n",
    "            episode: episode number\n",
    "            eval: if False, uses epsilon-greedy action selection, otherwise greedy\n",
    "        \"\"\"\n",
    "\n",
    "        if not eval:\n",
    "            explore = random.random()\n",
    "            if (explore < self.epsilon[episode]):\n",
    "                action = random.choice(list(range(self.num_actions)))\n",
    "                return action\n",
    "\n",
    "        self.model.q_1.eval()\n",
    "        with torch.no_grad():\n",
    "            curr_state = self._to_tensor(state)\n",
    "            curr_state = curr_state.unsqueeze(0)\n",
    "            action = torch.argmin(\n",
    "                    self.model.q_1(curr_state), dim=1).item()\n",
    "            if eval:\n",
    "                store_Q1 = [self.model.q_1(curr_state).squeeze(0)[i].item() for i in range(self.num_actions)]\n",
    "                store_Q_target1 = [self.model.q_target_1(curr_state).squeeze(0)[i].item() for i in range(self.num_actions)]\n",
    "                store_Q2 = [self.model.q_2(curr_state).squeeze(0)[i].item() for i in range(self.num_actions)]\n",
    "                store_Q_target2 = [self.model.q_target_2(curr_state).squeeze(0)[i].item() for i in range(self.num_actions)]\n",
    "        self.model.q_1.train()\n",
    "\n",
    "        return (action, store_Q1, store_Q_target1, store_Q2, store_Q_target2) if eval else action\n",
    "\n",
    "    def _update(self):\n",
    "        \"\"\"Updates q1, q2, q1_target and q2_target networks based on clipped Double Q Learning Algorithm\n",
    "        \"\"\"\n",
    "        if (len(self.buffer) < self.batch_size):\n",
    "            return\n",
    "        self.training_iter += 1\n",
    "        # Make sure actor_target and critic_target are in eval mode\n",
    "        assert not self.model.q_target_1.training\n",
    "        assert not self.model.q_target_2.training\n",
    "\n",
    "        assert self.model.q_1.training\n",
    "        assert self.model.q_2.training\n",
    "        transitions = self.buffer.sample(self.batch_size)\n",
    "        batch = self.buffer.transition(*zip(*transitions))\n",
    "        state_batch = self._to_tensor(batch.state)\n",
    "        action_batch = self._to_tensor(\n",
    "            batch.action).unsqueeze(1).to(torch.int64)\n",
    "        reward_batch = self._to_tensor(batch.reward)\n",
    "        next_state_batch = self._to_tensor(batch.next_state)\n",
    "        terminal = self._to_tensor(batch.terminal)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Add noise to smooth out learning\n",
    "            Q_next_1 = torch.min(self.model.q_target_1(\n",
    "                next_state_batch), dim=1)[0].unsqueeze(1)\n",
    "            Q_next_2 = torch.min(self.model.q_target_2(\n",
    "                next_state_batch), dim=1)[0].unsqueeze(1)\n",
    "            # Use max want to avoid underestimation bias #####\n",
    "            Q_next = torch.max(Q_next_1, Q_next_2)\n",
    "            Q_expected = reward_batch + self.gamma * Q_next * (1 - terminal)\n",
    "\n",
    "        Q_1 = self.model.q_1(state_batch).gather(1, action_batch)\n",
    "        Q_2 = self.model.q_2(state_batch).gather(1, action_batch)\n",
    "        L_1 = nn.MSELoss()(Q_1, Q_expected)\n",
    "        L_2 = nn.MSELoss()(Q_2, Q_expected)\n",
    "\n",
    "        self.loss.append([L_1.item(), L_2.item()])\n",
    "        self.model.q_optimizer_1.zero_grad()\n",
    "        self.model.q_optimizer_2.zero_grad()\n",
    "        L_1.backward()\n",
    "        L_2.backward()\n",
    "        self.model.q_optimizer_1.step()\n",
    "        self.model.q_optimizer_2.step()\n",
    "\n",
    "        # Q_1_new = self.model.q_1(state_batch).gather(1, action_batch)\n",
    "        # print(\"  %.2f\"%Q_expected.min().item())\n",
    "        # print(\"    %.2f\"%(Q_1_new - Q_1).min().item())\n",
    "        # self.store_Q.append([Q_1.tolist(), Q_2.tolist(), Q_expected.tolist()])\n",
    "\n",
    "        if (self.training_iter % self.update_freq) == 0:\n",
    "            self.model.update_target_nn()\n",
    "        self.model.grad_update_num +=1\n",
    "\n",
    "\n",
    "    def train(self, sweep_var, num_decisions=50): #200\n",
    "        \"\"\"Train q networks\n",
    "        Args:\n",
    "            num_decisions: Number of decisions to train algorithm for\n",
    "        \"\"\"\n",
    "        self.folder_name = \"./Results\" + str(sweep_var)\n",
    "        os.system(\"mkdir \" + self.folder_name)\n",
    "        self.update_plot = DynamicUpdate(self.delta_t,self.delay_embed_len,self.folder_name)\n",
    "\n",
    "        episodes = 50 #400\n",
    "        e = np.arange(episodes)\n",
    "        T = 300 # 380, choosing how fast to move from exploration to exploitation\n",
    "        eps = -np.log10(e/T)\n",
    "        self.epsilon = np.clip(eps,0.05,1) # clipping to ensure all values are between 0 and 1\n",
    "\n",
    "        for i in range(self.episode_num,episodes):\n",
    "            print(\"Episode: \", i)\n",
    "\n",
    "            # warmup\n",
    "            b = self.init\n",
    "            state = [0]*self.delay_embed_len*self.embed_multiplier\n",
    "            self.sim_controller.initialize(b)\n",
    "            _, cell_count = self.sim_controller.simulate_population(self.sim_controller.num_cells_init, b)\n",
    "            for k in range(1,36+self.delay_embed_len):\n",
    "                _, cell_count = self.sim_controller.simulate_population(cell_count[-1], b)\n",
    "                if k >= 36:\n",
    "                    state, _, _ = self.get_state_reward(state, cell_count, b)\n",
    "\n",
    "            for j in range(num_decisions):\n",
    "                action_index = self._get_action(state, i)\n",
    "                transition_to_add = [copy.deepcopy(state), action_index]\n",
    "\n",
    "                action_b = self.b_actions[self.b_index[action_index]]\n",
    "\n",
    "                _, cell_count = self.sim_controller.simulate_population(cell_count[-1], action_b)\n",
    "                state, reward, terminal = self.get_state_reward(state, cell_count, action_b/max(self.b_actions))\n",
    "                transition_to_add.extend([[reward], copy.deepcopy(state), [terminal]])\n",
    "                self.buffer.push(*list(transition_to_add))\n",
    "                self._update()\n",
    "                if cell_count[-1] == 0 or cell_count[-1] > self.max_pop:\n",
    "                    break\n",
    "\n",
    "            if (i % 10 == 0) or (i == episodes-1):\n",
    "                self.eval(i)\n",
    "                self._save_data(i)\n",
    "                if i == episodes-1:\n",
    "                    plot_reward_Q_loss(self.ave_sum_rewards, self.std_sum_rewards, self.grad_updates, self.loss, self.update_plot.folder_name_test,\n",
    "                                   self.ave_Q1, self.ave_Q2, self.ave_Q1_target, self.ave_Q2_target)\n",
    "\n",
    "\n",
    "\n",
    "    def eval(self, episode, num_decisions=50, num_evals=30): #200, 50\n",
    "        \"\"\"Given trained q networks, generate trajectories\n",
    "        \"\"\"\n",
    "        print(\"Evaluation\")\n",
    "\n",
    "        extinct_times = []\n",
    "        extinct_count = 0\n",
    "        max_cross_corr = []\n",
    "        lag = []\n",
    "\n",
    "        results = Parallel(n_jobs=-1)(delayed(self.rollout)(num_decisions) for i in range(num_evals))\n",
    "        # results = [self.rollout(num_decisions) for i in range(num_evals)]\n",
    "\n",
    "        i=0\n",
    "        for r in results:\n",
    "            if i == 0:\n",
    "                b_all, cell_count_all, t_all, kn0_all, rewards_all, Q1, Q1_target, Q2, Q2_target = r\n",
    "                sum_reward = np.array([rewards_all.sum()])\n",
    "                min_Q1 = np.array(Q1.min(axis=1))\n",
    "                min_Q1_target = np.array(Q1_target.min(axis=1))\n",
    "                min_Q2 = np.array(Q2.min(axis=1))\n",
    "                min_Q2_target = np.array(Q2_target.min(axis=1))\n",
    "                if not np.all(cell_count_all > 0):\n",
    "                    ind = np.where(cell_count_all == 0)[0][0]\n",
    "                    extinct_times.append(t_all[ind])\n",
    "                    extinct_count +=1\n",
    "                    ind +=1\n",
    "                else:\n",
    "                    ind = b_all.size\n",
    "                # compute max cross correlation and lag\n",
    "                if np.std(b_all[:ind]) > 0:\n",
    "                    n_points = ind\n",
    "                    cross_corr = signal.correlate(b_all[:ind] - np.mean(b_all[:ind]), kn0_all[:ind] - np.mean(kn0_all[:ind]), mode='full')\n",
    "                    cross_corr /= (np.std(b_all[:ind]) * np.std(kn0_all[:ind]) * n_points)  # Normalize\n",
    "                    max_cross_corr.append(np.max(cross_corr))\n",
    "                    lags = signal.correlation_lags(b_all[:ind].size,kn0_all[:ind].size, mode=\"full\")\n",
    "                    lag.append(lags[np.argmax(cross_corr)])\n",
    "                i+=1\n",
    "            else:\n",
    "                b, cell_count, t, kn0, rewards, Q1, Q1_target, Q2, Q2_target = r\n",
    "                b_all = np.concatenate((b_all, b), axis=1)\n",
    "                cell_count_all = np.concatenate((cell_count_all, cell_count), axis=1)\n",
    "                t_all = np.concatenate((t_all,t), axis=1)\n",
    "                kn0_all = np.concatenate((kn0_all,kn0), axis=1)\n",
    "                rewards_all = np.concatenate((rewards_all,rewards), axis=1)\n",
    "                sum_reward = np.concatenate((sum_reward, np.array([rewards.sum()])))\n",
    "                min_Q1 = np.vstack((min_Q1, np.array(Q1.min(axis=1))))\n",
    "                min_Q1_target = np.vstack((min_Q1_target, np.array(Q1_target.min(axis=1))))\n",
    "                min_Q2 = np.vstack((min_Q2, np.array(Q2.min(axis=1))))\n",
    "                min_Q2_target = np.vstack((min_Q2_target, np.array(Q2_target.min(axis=1))))\n",
    "                if not np.all(cell_count > 0):\n",
    "                    ind = np.where(cell_count == 0)[0][0]\n",
    "                    extinct_times.append(t[ind])\n",
    "                    extinct_count +=1\n",
    "                    ind +=1\n",
    "                else:\n",
    "                    ind = b.size\n",
    "                # compute max cross correlation and lag\n",
    "                if np.std(b_all[:ind]) > 0:\n",
    "                    n_points = ind\n",
    "                    cross_corr = signal.correlate(b[:ind] - np.mean(b[:ind]), kn0[:ind] - np.mean(kn0[:ind]), mode='full')\n",
    "                    cross_corr /= (np.std(b[:ind]) * np.std(kn0[:ind]) * n_points)  # Normalize\n",
    "                    max_cross_corr.append(np.max(cross_corr))\n",
    "                    lags = signal.correlation_lags(b[:ind].size,kn0[:ind].size, mode=\"full\")\n",
    "                    lag.append(lags[np.argmax(cross_corr)])\n",
    "\n",
    "        self.ave_sum_rewards.append(sum_reward.mean())\n",
    "        self.std_sum_rewards.append(sum_reward.std())\n",
    "        self.ave_Q1.append(min_Q1.mean())\n",
    "        self.ave_Q2.append(min_Q2.mean())\n",
    "        self.ave_Q1_target.append(min_Q1_target.mean())\n",
    "        self.ave_Q2_target.append(min_Q2_target.mean())\n",
    "        self.grad_updates.append(self.model.grad_update_num)\n",
    "        # save results\n",
    "        ave_ext_time = sum(extinct_times)/len(extinct_times) if len(extinct_times) > 0 else np.Inf\n",
    "        ave_max_cross_corr = sum(max_cross_corr)/len(max_cross_corr) if len(max_cross_corr) > 0 else 0\n",
    "        ave_corr_lag = sum(lag)/len(lag) if len(lag) > 0 else 0\n",
    "\n",
    "        # log via wandb\n",
    "        # wandb.log({\"extinct_fraction\": extinct_count/num_evals,\n",
    "        #     \"ave_ext_rate\": 1/ave_ext_time,\n",
    "        #     \"ave_max_cross_corr\": ave_max_cross_corr,\n",
    "        #     \"ave_corr_lag\": ave_corr_lag,\n",
    "        #     \"ave total reward\": sum_reward.mean(),\n",
    "        #     \"ave min Q1\": min_Q1.mean()})\n",
    "        # select five trajectories randomly to plot\n",
    "        rand_i = random.sample(range(num_evals), 5)\n",
    "        self.update_plot(episode, t_all[:,rand_i], cell_count_all[:,rand_i], kn0_all[:,rand_i], b_all[:,rand_i])\n",
    "\n",
    "\n",
    "    def rollout(self, num_decisions):\n",
    "        b_all = np.zeros((num_decisions+self.delay_embed_len,1))\n",
    "        cell_count_all = np.zeros((num_decisions+self.delay_embed_len,1))\n",
    "        t_all = np.zeros((num_decisions+self.delay_embed_len,1))\n",
    "        kn0_all = np.zeros((num_decisions+self.delay_embed_len,1))\n",
    "        rewards_all = np.zeros((num_decisions+self.delay_embed_len,1))\n",
    "        Q1_all = np.zeros((num_decisions+self.delay_embed_len,2))\n",
    "        Q1_target_all = np.zeros((num_decisions+self.delay_embed_len,2))\n",
    "        Q2_all = np.zeros((num_decisions+self.delay_embed_len,2))\n",
    "        Q2_target_all = np.zeros((num_decisions+self.delay_embed_len,2))\n",
    "\n",
    "        # warmup\n",
    "        b = self.init\n",
    "        state = [0]*self.delay_embed_len*self.embed_multiplier\n",
    "        self.sim_controller.initialize(b)\n",
    "        _, cell_count = self.sim_controller.simulate_population(self.sim_controller.num_cells_init, b)\n",
    "        for k in range(1,36+self.delay_embed_len):\n",
    "            t, cell_count = self.sim_controller.simulate_population(cell_count[-1], b)\n",
    "            if k >= 36:\n",
    "                _, Q1_all[k-36,:], Q1_target_all[k-36,:], Q2_all[k-36,:], Q2_target_all[k-36,:] = self._get_action(state, eval=True) # just calling this to save Q value for plot\n",
    "                state, rewards_all[k-36], _ = self.get_state_reward(state, cell_count, b)\n",
    "\n",
    "                b_all[k-36] = b\n",
    "                cell_count_all[k-36] = cell_count[-1]\n",
    "                t_all[k-36] = t[-1]\n",
    "                kn0_all[k-36] = self.sim_controller.k_n0\n",
    "\n",
    "        for j in range(num_decisions):\n",
    "            action_index, Q1_all[j+self.delay_embed_len,:], Q1_target_all[j+self.delay_embed_len,:], Q2_all[j+self.delay_embed_len,:], Q2_target_all[j+self.delay_embed_len,:] = self._get_action(state, eval=True)\n",
    "            action_b = self.b_actions[self.b_index[action_index]]\n",
    "\n",
    "            t, cell_count = self.sim_controller.simulate_population(cell_count[-1], action_b)\n",
    "            state, rewards_all[j+self.delay_embed_len], _ = self.get_state_reward(state, cell_count, action_b/max(self.b_actions))\n",
    "\n",
    "            b_all[j+self.delay_embed_len] = action_b\n",
    "            cell_count_all[j+self.delay_embed_len] = cell_count[-1]\n",
    "            t_all[j+self.delay_embed_len] = t[-1]\n",
    "            kn0_all[j+self.delay_embed_len] = self.sim_controller.k_n0\n",
    "            if cell_count[-1] == 0 or cell_count[-1] > self.max_pop:\n",
    "                break\n",
    "\n",
    "        return b_all, cell_count_all, t_all, kn0_all, rewards_all, Q1_all, Q1_target_all, Q2_all, Q2_target_all\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pass\n",
    "    # torch.manual_seed(0)\n",
    "    # c = CDQL()\n",
    "    # # c.load_data()\n",
    "    # c.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: ./Results12: File exists\n",
      "mkdir: ./Results12/Eval: File exists\n",
      "/var/folders/_2/b28clpbs6t36hp6grpxntpq40000gn/T/ipykernel_39652/3460861874.py:203: RuntimeWarning: divide by zero encountered in log10\n",
      "  eps = -np.log10(e/T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0\n",
      "Evaluation\n",
      "Episode:  1\n",
      "Episode:  2\n",
      "Episode:  3\n",
      "Episode:  4\n",
      "Episode:  5\n",
      "Episode:  6\n",
      "Episode:  7\n",
      "Episode:  8\n",
      "Episode:  9\n",
      "Episode:  10\n",
      "Evaluation\n",
      "Episode:  11\n",
      "Episode:  12\n",
      "Episode:  13\n",
      "Episode:  14\n",
      "Episode:  15\n",
      "Episode:  16\n",
      "Episode:  17\n",
      "Episode:  18\n",
      "Episode:  19\n",
      "Episode:  20\n",
      "Evaluation\n",
      "Episode:  21\n",
      "Episode:  22\n",
      "Episode:  23\n",
      "Episode:  24\n",
      "Episode:  25\n",
      "Episode:  26\n",
      "Episode:  27\n",
      "Episode:  28\n",
      "Episode:  29\n",
      "Episode:  30\n",
      "Evaluation\n",
      "Episode:  31\n",
      "Episode:  32\n",
      "Episode:  33\n",
      "Episode:  34\n",
      "Episode:  35\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m c \u001b[38;5;241m=\u001b[39m CDQL()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# c.load_data()\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 225\u001b[0m, in \u001b[0;36mCDQL.train\u001b[0;34m(self, sweep_var, num_decisions)\u001b[0m\n\u001b[1;32m    221\u001b[0m transition_to_add \u001b[38;5;241m=\u001b[39m [copy\u001b[38;5;241m.\u001b[39mdeepcopy(state), action_index]\n\u001b[1;32m    223\u001b[0m action_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_actions[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_index[action_index]]\n\u001b[0;32m--> 225\u001b[0m _, cell_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msim_controller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulate_population\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell_count\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_b\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m state, reward, terminal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_state_reward(state, cell_count, action_b\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_actions))\n\u001b[1;32m    227\u001b[0m transition_to_add\u001b[38;5;241m.\u001b[39mextend([[reward], copy\u001b[38;5;241m.\u001b[39mdeepcopy(state), [terminal]])\n",
      "File \u001b[0;32m~/Documents/JUPYTER NOTEBOOK/RL-for-bacterial-control/cell_model_full_parallel.py:255\u001b[0m, in \u001b[0;36mCell_Population.simulate_population\u001b[0;34m(self, true_num_cells, b, n_steps, threshold)\u001b[0m\n\u001b[1;32m    253\u001b[0m species_stack \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([phiR_birth, phiS_birth, a_birth, U_birth, X_birth, V_birth])\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, iterations):\n\u001b[0;32m--> 255\u001b[0m     species_stack \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultiIntegrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspecies_stack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_n0\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# integrating one timestep\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     X_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# amount of division proteins required to trigger division\u001b[39;00m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# if cell has added threshold volume amount, it will then divide\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/JUPYTER NOTEBOOK/RL-for-bacterial-control/cell_model_full_parallel.py:128\u001b[0m, in \u001b[0;36mCell_Population.MultiIntegrate\u001b[0;34m(self, Species, t, dt, b, k_n0)\u001b[0m\n\u001b[1;32m    125\u001b[0m     U[:] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    126\u001b[0m U[U \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 128\u001b[0m X \u001b[38;5;241m=\u001b[39m X_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdXdt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphiR_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mU_i\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m*\u001b[39mdt\n\u001b[1;32m    129\u001b[0m V \u001b[38;5;241m=\u001b[39m V_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdVdt(V_i, t, a_i, phiR_i, U_i)\u001b[38;5;241m*\u001b[39mdt\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([phi_R,phi_S,a,U,X,V])\n",
      "File \u001b[0;32m~/Documents/JUPYTER NOTEBOOK/RL-for-bacterial-control/cell_model_full_parallel.py:93\u001b[0m, in \u001b[0;36mCell_Population.dXdt\u001b[0;34m(self, X, t, a, phi_R, V, U)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdXdt\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, t, a, phi_R, V, U):\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# division protein ODE\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     dxdt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_X\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mU\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGrowthRate(a, phi_R, U) \u001b[38;5;241m*\u001b[39m V \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmu \u001b[38;5;241m*\u001b[39m X\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dxdt\n",
      "File \u001b[0;32m~/Documents/JUPYTER NOTEBOOK/RL-for-bacterial-control/cell_model_full_parallel.py:55\u001b[0m, in \u001b[0;36mCell_Population.f_X\u001b[0;34m(self, a, U)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf_X\u001b[39m(\u001b[38;5;28mself\u001b[39m, a, U):\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphaX\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphiR_max \u001b[38;5;241m-\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_R\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mU\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbetaX\n",
      "File \u001b[0;32m~/Documents/JUPYTER NOTEBOOK/RL-for-bacterial-control/cell_model_full_parallel.py:52\u001b[0m, in \u001b[0;36mCell_Population.f_R\u001b[0;34m(self, a, U)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf_R\u001b[39m(\u001b[38;5;28mself\u001b[39m, a, U):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_prime(a)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg(a)\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphiR_max\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_S(U)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf(a)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_prime(a)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphiR_min) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_prime\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg(a) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf(a)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_prime(a))\n",
      "File \u001b[0;32m~/Documents/JUPYTER NOTEBOOK/RL-for-bacterial-control/cell_model_full_parallel.py:40\u001b[0m, in \u001b[0;36mCell_Population.f_prime\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(\u001b[38;5;28mself\u001b[39m, a):\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m (a\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma_n)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_f) \u001b[38;5;66;03m# regulatory function for k_n\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf_prime\u001b[39m(\u001b[38;5;28mself\u001b[39m, a):\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_f\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma_n)\u001b[38;5;241m*\u001b[39m(a\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma_n)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_f\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m (a\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma_n)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_f)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;66;03m# derivative of f w.r.t. a\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mg\u001b[39m(\u001b[38;5;28mself\u001b[39m, a):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "c = CDQL()\n",
    "# c.load_data()\n",
    "c.train(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
