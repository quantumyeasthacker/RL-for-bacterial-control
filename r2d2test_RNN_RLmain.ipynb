{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cell_model_full_parallel import Cell_Population\n",
    "from r2d2test_RNN_replaybuffer import ReplayBuffer\n",
    "from r2d2test_RNN_deepQLnetwork import Model\n",
    "from joblib import Parallel, delayed\n",
    "from utils_figure_plot import DynamicUpdate, plot_reward_Q_loss\n",
    "import copy\n",
    "from scipy import signal\n",
    "import matplotlib as mpl\n",
    "# import wandb\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "\n",
    "class CDQL:\n",
    "    def __init__(self, batch_size=32,\n",
    "        delta_t=0.2,\n",
    "        omega=0.02,\n",
    "        gamma=0.99,\n",
    "        update_freq=2,\n",
    "        use_gpu=False,\n",
    "        num_cells_init=60,\n",
    "        kn0_mean=2.55,\n",
    "        T=12,\n",
    "        agent_input=\"full\"):\n",
    "\n",
    "        self.gamma = gamma\n",
    "        if use_gpu and torch.cuda.is_available(): # and torch.cuda.device_count() > 1:\n",
    "            self.device = torch.device('cuda')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "        assert (not use_gpu) or (self.device == torch.device('cuda'))\n",
    "\n",
    "        self.delta_t = delta_t\n",
    "        self.sim_controller = Cell_Population(num_cells_init, delta_t, omega, kn0_mean, T)\n",
    "        self.buffer = ReplayBuffer(1e6)\n",
    "        self.batch_size = batch_size\n",
    "        self.b_actions = [0.0, 3.72]\n",
    "        self.b_num_actions = len(self.b_actions)\n",
    "        self.num_actions = self.b_num_actions\n",
    "        self.b_index = [0, 1]\n",
    "        self.max_seq_len = 4\n",
    "\n",
    "        if agent_input == \"full\":\n",
    "            self.get_state_reward = self.sim_controller.get_reward_all\n",
    "            self.embed_multiplier = 3\n",
    "        elif agent_input == \"no_nutrient\":\n",
    "            self.get_state_reward = self.sim_controller.get_reward_no_nutrient\n",
    "            self.embed_multiplier = 2\n",
    "        elif agent_input == \"no_act\":\n",
    "            self.get_state_reward = self.sim_controller.get_reward_no_antibiotic\n",
    "            self.embed_multiplier = 2\n",
    "        else:\n",
    "            raise ValueError(\"Invalid agent_input value.\")\n",
    "\n",
    "        self.model = Model(self.device, num_inputs=self.embed_multiplier, num_actions=self.num_actions)\n",
    "\n",
    "        self.init = (0.0, [None]) # (b, hidden_state)\n",
    "        self.loss = []\n",
    "        self.ave_sum_rewards = []\n",
    "        self.std_sum_rewards = []\n",
    "        # self.ave_Q1 = []\n",
    "        # self.ave_Q2 = []\n",
    "        # self.ave_Q1_target = []\n",
    "        # self.ave_Q2_target = []\n",
    "        self.grad_updates = []\n",
    "        self.training_iter = 0\n",
    "        self.update_freq = update_freq\n",
    "        self.epsilon = None\n",
    "        self.episode_num = 0\n",
    "\n",
    "    def _to_tensor(self, x):\n",
    "        return torch.tensor(x).float().to(self.device)\n",
    "\n",
    "    def _save_data(self, episode_num):\n",
    "        np.save(self.folder_name + \"/replaybuffer\", np.array(self.buffer.buffer, dtype=object))\n",
    "\n",
    "        self.model.save_networks(self.folder_name+'/')\n",
    "        self.save_episode_num(episode_num)\n",
    "\n",
    "    def save_episode_num(self, episode_num, filename='/episode_num.txt'):\n",
    "        filename = self.folder_name + filename\n",
    "        with open(filename,\"w\") as file:\n",
    "            file.write(str(episode_num))\n",
    "\n",
    "    def load_data(self, sweep_var, folder_name=\"./Results\"):\n",
    "        self.folder_name = folder_name + str(sweep_var)\n",
    "        self.buffer.load_buffer(self.folder_name + \"/replaybuffer.npy\")\n",
    "        self.model.load_networks(self.folder_name)\n",
    "        self.episode_num = self.load_episode_num()\n",
    "\n",
    "    def load_episode_num(self, filename='/episode_num.txt'):\n",
    "        filename = self.folder_name + filename\n",
    "        try:\n",
    "            with open(filename, \"r\") as file:\n",
    "                episode_num = int(file.read().strip()) + 1\n",
    "            print(\"Partially trained model found, starting from episode \",episode_num,\".\")\n",
    "            return episode_num\n",
    "        except FileNotFoundError:\n",
    "            return 0\n",
    "\n",
    "    def _get_action(self, state, hidden_state, episode=None, eval=False):\n",
    "        \"\"\"Gets action given some state\n",
    "        if episode is less than 5 returns a random action for each region\n",
    "        Args:\n",
    "            state: List defining the state\n",
    "            hidden_state: List containing previous LSTM hidden state\n",
    "            episode: episode number\n",
    "        \"\"\"\n",
    "\n",
    "        self.model.q_1.eval()\n",
    "        if not eval:\n",
    "            explore = random.random()\n",
    "\n",
    "            if (explore < self.epsilon[episode]):\n",
    "                with torch.no_grad():\n",
    "                    curr_state = self._to_tensor(state)\n",
    "                    curr_state = curr_state.unsqueeze(0)\n",
    "                    x = self.model.q_1({'obs':curr_state, 'prev_state': hidden_state}, inference=True)\n",
    "                    action = random.choice(list(range(self.num_actions)))\n",
    "                self.model.q_1.train()\n",
    "                return action, x['next_state']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            curr_state = self._to_tensor(state)\n",
    "            curr_state = curr_state.unsqueeze(0)\n",
    "            x = self.model.q_1({'obs':curr_state, 'prev_state': hidden_state}, inference=True)\n",
    "            action = torch.argmin(x['logit'], dim=1).item()\n",
    "            # add storage of Q values later, modifying below\n",
    "            # if eval:\n",
    "                # store_Q1 = [self.model.q_1(curr_state).squeeze(0)[i].item() for i in range(self.num_actions)]\n",
    "                # store_Q_target1 = [self.model.q_target_1(curr_state).squeeze(0)[i].item() for i in range(self.num_actions)]\n",
    "                # store_Q2 = [self.model.q_2(curr_state).squeeze(0)[i].item() for i in range(self.num_actions)]\n",
    "                # store_Q_target2 = [self.model.q_target_2(curr_state).squeeze(0)[i].item() for i in range(self.num_actions)]\n",
    "        self.model.q_1.train()\n",
    "        return action, x['next_state']\n",
    "\n",
    "    def _update(self):\n",
    "        \"\"\"Updates q1, q2, q1_target and q2_target networks based on clipped Double Q Learning Algorithm\n",
    "        \"\"\"\n",
    "        if (len(self.buffer) < self.batch_size):\n",
    "            return\n",
    "        self.training_iter += 1\n",
    "        # Make sure actor_target and critic_target are in eval mode\n",
    "        assert not self.model.q_target_1.training\n",
    "        assert not self.model.q_target_2.training\n",
    "\n",
    "        assert self.model.q_1.training\n",
    "        assert self.model.q_2.training\n",
    "        transitions = self.buffer.sample(self.batch_size)\n",
    "        batch = self.buffer.transition(*zip(*transitions))\n",
    "        state_batch = self._to_tensor(batch.state)\n",
    "        hidden_state = [hidden for hidden, *_ in batch.hidden_state_init] # needed to properly convert tuple to list\n",
    "\n",
    "        action_batch = self._to_tensor(\n",
    "            batch.action).transpose(0,1).unsqueeze(2).to(torch.int64)\n",
    "        reward_batch = self._to_tensor(batch.reward).transpose(0,1).unsqueeze(2)\n",
    "        next_state_batch = self._to_tensor(batch.next_state)\n",
    "\n",
    "        Q_1 = self.model.q_1(\n",
    "            {'obs':state_batch, 'prev_state': hidden_state})\n",
    "        Q_2 = self.model.q_2(\n",
    "            {'obs':state_batch, 'prev_state': hidden_state})\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Add noise to smooth out learning\n",
    "            Q_next_1 = torch.min(self.model.q_target_1(\n",
    "                {'obs':next_state_batch, 'prev_state': hidden_state})['logit'], dim=2)[0].unsqueeze(2)\n",
    "            Q_next_2 = torch.min(self.model.q_target_2(\n",
    "                {'obs':next_state_batch, 'prev_state': hidden_state})['logit'], dim=2)[0].unsqueeze(2)\n",
    "            # Use max want to avoid underestimation bias\n",
    "            Q_next = torch.max(Q_next_1, Q_next_2)\n",
    "            Q_expected = reward_batch + self.gamma * Q_next  # No \"Terminal State\"\n",
    "\n",
    "        Q_1 = Q_1['logit'].gather(2, action_batch)\n",
    "        Q_2 = Q_2['logit'].gather(2, action_batch)\n",
    "        L_1 = nn.MSELoss()(Q_1, Q_expected)\n",
    "        L_2 = nn.MSELoss()(Q_2, Q_expected)\n",
    "\n",
    "        self.loss.append([L_1.item(), L_2.item()])\n",
    "        self.model.q_optimizer_1.zero_grad()\n",
    "        self.model.q_optimizer_2.zero_grad()\n",
    "        L_1.backward()\n",
    "        L_2.backward()\n",
    "        self.model.q_optimizer_1.step()\n",
    "        self.model.q_optimizer_2.step()\n",
    "        if (self.training_iter % self.update_freq) == 0:\n",
    "            self.model.update_target_nn()\n",
    "        self.model.grad_update_num +=1\n",
    "\n",
    "\n",
    "    def train(self, sweep_var, num_decisions=10):\n",
    "        \"\"\"Train q networks\n",
    "        Args:\n",
    "            num_decisions: Number of decisions to train algorithm for\n",
    "        \"\"\"\n",
    "        self.folder_name = \"./Results\" + str(sweep_var)\n",
    "        os.system(\"mkdir \" + self.folder_name)\n",
    "        self.update_plot = DynamicUpdate(self.delta_t,self.folder_name)\n",
    "\n",
    "        episodes = 10\n",
    "        e = np.arange(episodes)\n",
    "        T = 9 # choosing how fast to move from exploration to exploitation\n",
    "        eps = -np.log10(e/T)\n",
    "        self.epsilon = np.clip(eps,0.05,1) # clipping to ensure all values are between 0 and 1\n",
    "\n",
    "        for i in range(self.episode_num,episodes):\n",
    "            print(\"Episode: \", i)\n",
    "            # episode_folder_name = self.folder_name + \"Train/\" + str(i) + \"/\"\n",
    "\n",
    "            # os.system(\"mkdir \" + episode_folder_name)\n",
    "            # filename = episode_folder_name + str(i)\n",
    "\n",
    "            # warmup\n",
    "            b, hidden_state = self.init\n",
    "            state = [0]*self.embed_multiplier\n",
    "            self.sim_controller.initialize(b)\n",
    "            _, cell_count = self.sim_controller.simulate_population(self.sim_controller.num_cells_init, b)\n",
    "            for k in range(1,37):\n",
    "                _, cell_count = self.sim_controller.simulate_population(cell_count[-1], b)\n",
    "                if k == 36:\n",
    "                    state, _ = self.get_state_reward(state, cell_count, b)\n",
    "\n",
    "            state_seq = []\n",
    "            action_seq = []\n",
    "            reward_seq = []\n",
    "            next_state_seq = []\n",
    "            self.it = 1\n",
    "\n",
    "            for j in range(num_decisions):\n",
    "                state_seq.append(copy.deepcopy(state))\n",
    "                if self.it == 1:\n",
    "                    hidden_state_init = hidden_state\n",
    "                action_index, hidden_state = self._get_action(state, hidden_state, i)\n",
    "                action_seq.append(action_index)\n",
    "\n",
    "                action_b = self.b_actions[self.b_index[action_index]]\n",
    "\n",
    "                _, cell_count = self.sim_controller.simulate_population(cell_count[-1], action_b)\n",
    "                state, reward = self.get_state_reward(state, cell_count, action_b/max(self.b_actions))\n",
    "\n",
    "                reward_seq.append(reward)\n",
    "                next_state_seq.append(copy.deepcopy(state))\n",
    "\n",
    "                if self.it < self.max_seq_len:\n",
    "                    self.it +=1\n",
    "                else:\n",
    "                    # print('state',state_seq)\n",
    "                    # print('act',action_seq)\n",
    "                    # print('next',next_state_seq)\n",
    "                    transition_to_add = [state_seq, action_seq, reward_seq, next_state_seq, hidden_state_init]\n",
    "                    self.it = 1\n",
    "                    state_seq = []\n",
    "                    action_seq = []\n",
    "                    reward_seq = []\n",
    "                    next_state_seq = []\n",
    "\n",
    "                    self.buffer.push(*list(transition_to_add))\n",
    "                    if cell_count[-1] == 0:\n",
    "                        self._update()\n",
    "                        break\n",
    "                self._update()\n",
    "\n",
    "            if (i % 10 == 0) or (i == episodes-1):\n",
    "                self.eval(i)\n",
    "                self._save_data(i)\n",
    "\n",
    "\n",
    "\n",
    "    def eval(self, episode, num_decisions=5, num_evals=5): #200, 50\n",
    "        \"\"\"Given trained q networks, generate trajectories\n",
    "        \"\"\"\n",
    "        print(\"Evaluation\")\n",
    "\n",
    "        extinct_times = []\n",
    "        extinct_count = 0\n",
    "        max_cross_corr = []\n",
    "        lag = []\n",
    "\n",
    "        results = Parallel(n_jobs=-1)(delayed(self.rollout)(num_decisions) for i in range(num_evals))\n",
    "        # results = [self.rollout(num_decisions) for i in range(num_evals)]\n",
    "\n",
    "        i=0\n",
    "        for r in results:\n",
    "            if i == 0:\n",
    "                b_all, cell_count_all, t_all, kn0_all, rewards_all = r\n",
    "                sum_reward = np.array([rewards_all.sum()])\n",
    "                # min_Q1 = np.array(Q1.min(axis=1))\n",
    "                # min_Q1_target = np.array(Q1_target.min(axis=1))\n",
    "                # min_Q2 = np.array(Q2.min(axis=1))\n",
    "                # min_Q2_target = np.array(Q2_target.min(axis=1))\n",
    "                if not np.all(cell_count_all > 0):\n",
    "                    ind = np.where(cell_count_all == 0)[0][0]\n",
    "                    extinct_times.append(t_all[ind])\n",
    "                    extinct_count +=1\n",
    "                    ind +=1\n",
    "                else:\n",
    "                    ind = b_all.size\n",
    "                # compute max cross correlation and lag\n",
    "                if np.std(b_all[:ind]) > 0:\n",
    "                    n_points = ind\n",
    "                    cross_corr = signal.correlate(b_all[:ind] - np.mean(b_all[:ind]), kn0_all[:ind] - np.mean(kn0_all[:ind]), mode='full')\n",
    "                    cross_corr /= (np.std(b_all[:ind]) * np.std(kn0_all[:ind]) * n_points)  # Normalize\n",
    "                    max_cross_corr.append(np.max(cross_corr))\n",
    "                    lags = signal.correlation_lags(b_all[:ind].size,kn0_all[:ind].size, mode=\"full\")\n",
    "                    lag.append(lags[np.argmax(cross_corr)])\n",
    "                i+=1\n",
    "            else:\n",
    "                b, cell_count, t, kn0, rewards = r\n",
    "                b_all = np.concatenate((b_all, b), axis=1)\n",
    "                cell_count_all = np.concatenate((cell_count_all, cell_count), axis=1)\n",
    "                t_all = np.concatenate((t_all,t), axis=1)\n",
    "                kn0_all = np.concatenate((kn0_all,kn0), axis=1)\n",
    "                rewards_all = np.concatenate((rewards_all,rewards), axis=1)\n",
    "                sum_reward = np.concatenate((sum_reward, np.array([rewards.sum()])))\n",
    "                # min_Q1 = np.vstack((min_Q1, np.array(Q1.min(axis=1))))\n",
    "                # min_Q1_target = np.vstack((min_Q1_target, np.array(Q1_target.min(axis=1))))\n",
    "                # min_Q2 = np.vstack((min_Q2, np.array(Q2.min(axis=1))))\n",
    "                # min_Q2_target = np.vstack((min_Q2_target, np.array(Q2_target.min(axis=1))))\n",
    "                if not np.all(cell_count > 0):\n",
    "                    ind = np.where(cell_count == 0)[0][0]\n",
    "                    extinct_times.append(t[ind])\n",
    "                    extinct_count +=1\n",
    "                    ind +=1\n",
    "                else:\n",
    "                    ind = b.size\n",
    "                # compute max cross correlation and lag\n",
    "                if np.std(b[:ind]) > 0:\n",
    "                    n_points = ind\n",
    "                    cross_corr = signal.correlate(b[:ind] - np.mean(b[:ind]), kn0[:ind] - np.mean(kn0[:ind]), mode='full')\n",
    "                    cross_corr /= (np.std(b[:ind]) * np.std(kn0[:ind]) * n_points)  # Normalize\n",
    "                    max_cross_corr.append(np.max(cross_corr))\n",
    "                    lags = signal.correlation_lags(b[:ind].size,kn0[:ind].size, mode=\"full\")\n",
    "                    lag.append(lags[np.argmax(cross_corr)])\n",
    "\n",
    "        self.ave_sum_rewards.append(sum_reward.mean())\n",
    "        self.std_sum_rewards.append(sum_reward.std())\n",
    "        # self.ave_Q1.append(min_Q1.mean())\n",
    "        # self.ave_Q2.append(min_Q2.mean())\n",
    "        # self.ave_Q1_target.append(min_Q1_target.mean())\n",
    "        # self.ave_Q2_target.append(min_Q2_target.mean())\n",
    "        self.grad_updates.append(self.model.grad_update_num)\n",
    "        # save extinction quantification results\n",
    "        if len(extinct_times) > 0:\n",
    "            ave_ext_time = sum(extinct_times)/len(extinct_times)\n",
    "        else:\n",
    "            ave_ext_time = np.Inf\n",
    "\n",
    "        # log via wandb\n",
    "        # wandb.log({\"extinct_fraction\": extinct_count/num_evals,\n",
    "        #     \"ave_ext_rate\": 1/ave_ext_time,\n",
    "        #     \"ave_max_cross_corr\": sum(max_cross_corr)/len(max_cross_corr),\n",
    "        #     \"ave_corr_lag\": sum(lag)/len(lag),\n",
    "        #     \"ave total reward\": sum_reward.mean(),\n",
    "        #     \"ave min Q1\": min_Q1.mean()})\n",
    "        # select five trajectories randomly to plot\n",
    "        rand_i = random.sample(range(num_evals), 5)\n",
    "        self.update_plot(episode, t_all[:,rand_i], cell_count_all[:,rand_i], kn0_all[:,rand_i], b_all[:,rand_i])\n",
    "\n",
    "\n",
    "    def rollout(self, num_decisions):\n",
    "        b_all = np.zeros((num_decisions+1,1))\n",
    "        cell_count_all = np.zeros((num_decisions+1,1))\n",
    "        t_all = np.zeros((num_decisions+1,1))\n",
    "        kn0_all = np.zeros((num_decisions+1,1))\n",
    "        rewards_all = np.zeros((num_decisions+1,1))\n",
    "        # Q1_all = np.zeros((num_decisions+1,2))\n",
    "        # Q1_target_all = np.zeros((num_decisions+1,2))\n",
    "        # Q2_all = np.zeros((num_decisions+1,2))\n",
    "        # Q2_target_all = np.zeros((num_decisions+1,2))\n",
    "\n",
    "        # warmup\n",
    "        b, hidden_state = self.init\n",
    "        state = [0]*self.embed_multiplier\n",
    "        self.sim_controller.initialize(b)\n",
    "        _, cell_count = self.sim_controller.simulate_population(self.sim_controller.num_cells_init, b)\n",
    "        for k in range(1,36+1):\n",
    "            t, cell_count = self.sim_controller.simulate_population(cell_count[-1], b)\n",
    "            if k == 36:\n",
    "                # _, Q1_all[k-36,:], Q1_target_all[k-36,:], Q2_all[k-36,:], Q2_target_all[k-36,:] = self._get_action(state, eval=True) # just calling this to save Q value for plot\n",
    "                state, rewards_all[k-36] = self.get_state_reward(state, cell_count, b)\n",
    "\n",
    "                b_all[k-36] = b\n",
    "                cell_count_all[k-36] = cell_count[-1]\n",
    "                t_all[k-36] = t[-1]\n",
    "                kn0_all[k-36] = self.sim_controller.k_n0\n",
    "\n",
    "        for j in range(num_decisions):\n",
    "            action_index, hidden_state = self._get_action(state, hidden_state, eval=True)\n",
    "            action_b = self.b_actions[self.b_index[action_index]]\n",
    "\n",
    "            t, cell_count = self.sim_controller.simulate_population(cell_count[-1], action_b)\n",
    "            state, rewards_all[j+1] = self.get_state_reward(state, cell_count, action_b/max(self.b_actions))\n",
    "\n",
    "            b_all[j+1] = action_b\n",
    "            cell_count_all[j+1] = cell_count[-1]\n",
    "            t_all[j+1] = t[-1]\n",
    "            kn0_all[j+1] = self.sim_controller.k_n0\n",
    "            if cell_count[-1] == 0:\n",
    "                break\n",
    "\n",
    "        return b_all, cell_count_all, t_all, kn0_all, rewards_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: ./Results12: File exists\n",
      "mkdir: ./Results12/Eval: File exists\n",
      "/var/folders/_2/b28clpbs6t36hp6grpxntpq40000gn/T/ipykernel_66141/2871323450.py:210: RuntimeWarning: divide by zero encountered in log10\n",
      "  eps = -np.log10(e/T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0\n",
      "Evaluation\n",
      "Episode:  1\n",
      "Episode:  2\n",
      "Episode:  3\n",
      "Episode:  4\n",
      "Episode:  5\n",
      "Episode:  6\n",
      "Episode:  7\n",
      "Episode:  8\n",
      "Episode:  9\n",
      "Evaluation\n"
     ]
    }
   ],
   "source": [
    "c = CDQL()\n",
    "# c.load_data(12)\n",
    "c.train(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
