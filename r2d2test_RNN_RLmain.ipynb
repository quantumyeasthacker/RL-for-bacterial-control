{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from cell_model_full_parallel import Cell_Population\n",
    "from r2d2test_RNN_replaybuffer import ReplayBuffer\n",
    "from r2d2test_RNN_deepQLnetwork import Model\n",
    "from joblib import Parallel, delayed\n",
    "from utils_figure_plot_RNN import DynamicUpdate, plot_reward_Q_loss\n",
    "import copy\n",
    "from scipy import signal\n",
    "import matplotlib as mpl\n",
    "# import wandb\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "\n",
    "class CDQL:\n",
    "    def __init__(self, batch_size=16,\n",
    "        delta_t=0.2,\n",
    "        omega=0.02,\n",
    "        gamma=0.99,\n",
    "        update_freq=2,\n",
    "        use_gpu=False,\n",
    "        num_cells_init=60,\n",
    "        kn0_mean=2.55,\n",
    "        agent_input=\"full\",\n",
    "        training_config={}):\n",
    "\n",
    "        T = training_config[\"T\"]\n",
    "        self.max_seq_len = training_config[\"max_seq_len\"]\n",
    "        self.folder_name = training_config[\"folder_name\"]\n",
    "        os.makedirs(self.folder_name, exist_ok=True)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        if use_gpu and torch.cuda.is_available(): # and torch.cuda.device_count() > 1:\n",
    "            self.device = torch.device('cuda')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "        assert (not use_gpu) or (self.device == torch.device('cuda'))\n",
    "\n",
    "        self.delta_t = delta_t\n",
    "        self.sim_controller = Cell_Population(num_cells_init, delta_t, omega, kn0_mean, T)\n",
    "        self.buffer = ReplayBuffer(1e6)\n",
    "        self.batch_size = batch_size\n",
    "        self.b_actions = [0.0, 3.72]\n",
    "        self.b_num_actions = len(self.b_actions)\n",
    "        self.num_actions = self.b_num_actions\n",
    "        self.b_index = [0, 1]\n",
    "\n",
    "        if agent_input == \"full\":\n",
    "            self.get_state_reward = self.sim_controller.get_reward_all\n",
    "            self.embed_multiplier = 3\n",
    "        elif agent_input == \"no_nutrient\":\n",
    "            self.get_state_reward = self.sim_controller.get_reward_no_nutrient\n",
    "            self.embed_multiplier = 2\n",
    "        elif agent_input == \"no_act\":\n",
    "            self.get_state_reward = self.sim_controller.get_reward_no_antibiotic\n",
    "            self.embed_multiplier = 2\n",
    "        else:\n",
    "            raise ValueError(\"Invalid agent_input value.\")\n",
    "\n",
    "        self.model = Model(self.device, num_inputs=self.embed_multiplier, num_actions=self.num_actions)\n",
    "\n",
    "        self.init = (0.0, [None]) # (b, hidden_state)\n",
    "        self.max_pop = 1e11 # threshold for terminating episode\n",
    "        self.loss = []\n",
    "        self.ave_sum_rewards = []\n",
    "        self.std_sum_rewards = []\n",
    "        self.ave_Q1 = []\n",
    "        self.ave_Q2 = []\n",
    "        self.ave_Q1_target = []\n",
    "        self.ave_Q2_target = []\n",
    "        self.grad_updates = []\n",
    "        self.training_iter = 0\n",
    "        self.update_freq = update_freq\n",
    "        self.epsilon = None\n",
    "        self.episode_num = 0\n",
    "\n",
    "    def _to_tensor(self, x):\n",
    "        return torch.tensor(x).float().to(self.device)\n",
    "\n",
    "    def _save_data(self, episode_num):\n",
    "        np.save(self.folder_name + \"/replaybuffer\", np.array(self.buffer.buffer, dtype=object))\n",
    "\n",
    "        self.model.save_networks(self.folder_name+'/')\n",
    "        self.save_episode_num(episode_num)\n",
    "\n",
    "    def save_episode_num(self, episode_num, filename='/episode_num.txt'):\n",
    "        filename = self.folder_name + filename\n",
    "        with open(filename,\"w\") as file:\n",
    "            file.write(str(episode_num))\n",
    "\n",
    "    def load_data(self, sweep_var, folder_name=\"./Results\"):\n",
    "        self.folder_name = folder_name + str(sweep_var)\n",
    "        self.buffer.load_buffer(self.folder_name + \"/replaybuffer.npy\")\n",
    "        self.model.load_networks(self.folder_name)\n",
    "        self.episode_num = self.load_episode_num()\n",
    "\n",
    "    def load_episode_num(self, filename='/episode_num.txt'):\n",
    "        filename = self.folder_name + filename\n",
    "        try:\n",
    "            with open(filename, \"r\") as file:\n",
    "                episode_num = int(file.read().strip()) + 1\n",
    "            print(\"Partially trained model found, starting from episode \",episode_num,\".\")\n",
    "            return episode_num\n",
    "        except FileNotFoundError:\n",
    "            return 0\n",
    "\n",
    "    def _get_action(self, state, hidden_state, episode=None, eval=False):\n",
    "        \"\"\"Gets action given some state\n",
    "        if episode is less than 5 returns a random action for each region\n",
    "        Args:\n",
    "            state: List defining the state\n",
    "            hidden_state: List containing previous RNN hidden state\n",
    "            episode: episode number\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"simplfy this code below\n",
    "        - check that gradients are not being stored (try with action out and detach)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        self.model.q_1.eval()\n",
    "        with torch.no_grad():\n",
    "            curr_state = self._to_tensor(state)\n",
    "            curr_state = curr_state.unsqueeze(0)\n",
    "            x = self.model.q_1({'obs':curr_state, 'prev_state': hidden_state}, inference=True)\n",
    "            action = torch.argmin(x['logit'], dim=1).item()\n",
    "\n",
    "        if not eval:\n",
    "            explore = random.random()\n",
    "\n",
    "            if (explore < self.epsilon[episode]):\n",
    "                action = random.choice(list(range(self.num_actions)))\n",
    "                self.model.q_1.train()\n",
    "                return action, x['next_state']\n",
    "\n",
    "        if eval:\n",
    "            with torch.no_grad():\n",
    "                # store_Q1 = [self.model.q_1({'obs':curr_state, 'prev_state': hidden_state}, inference=True)['logit'].squeeze(0)[i].item() for i in range(self.num_actions)]\n",
    "                store_minQ1 = self.model.q_1({'obs':curr_state, 'prev_state': hidden_state}, inference=True)['logit'].min().numpy()\n",
    "                store_minQtarget1 = self.model.q_target_1({'obs':curr_state, 'prev_state': hidden_state}, inference=True)['logit'].min().numpy()\n",
    "                store_minQ2 = self.model.q_2({'obs':curr_state, 'prev_state': hidden_state}, inference=True)['logit'].min().numpy()\n",
    "                store_minQtarget2 = self.model.q_target_2({'obs':curr_state, 'prev_state': hidden_state}, inference=True)['logit'].min().numpy()\n",
    "        self.model.q_1.train()\n",
    "        return (action, x['next_state'], store_minQ1, store_minQtarget1, store_minQ2, store_minQtarget2) if eval else (action, x['next_state'])\n",
    "\n",
    "    def _update(self):\n",
    "        \"\"\"Updates q1, q2, q1_target and q2_target networks based on clipped Double Q Learning Algorithm\n",
    "        \"\"\"\n",
    "        if (len(self.buffer) < self.batch_size):\n",
    "            return\n",
    "        self.training_iter += 1\n",
    "        # Make sure actor_target and critic_target are in eval mode\n",
    "        assert not self.model.q_target_1.training\n",
    "        assert not self.model.q_target_2.training\n",
    "\n",
    "        assert self.model.q_1.training\n",
    "        assert self.model.q_2.training\n",
    "        transitions = self.buffer.sample(self.batch_size)\n",
    "        batch = self.buffer.transition(*zip(*transitions))\n",
    "        state_batch = self._to_tensor(batch.state)\n",
    "        hidden_state = [hidden for hidden, *_ in batch.hidden_state_init] # needed to properly convert tuple to list\n",
    "        next_hidden_state = [hidden for hidden, *_ in batch.next_hidden_state_init]\n",
    "\n",
    "        action_batch = self._to_tensor(\n",
    "            batch.action).transpose(0,1).unsqueeze(2).to(torch.int64)\n",
    "        reward_batch = self._to_tensor(batch.reward).transpose(0,1).unsqueeze(2)\n",
    "        next_state_batch = self._to_tensor(batch.next_state)\n",
    "        terminal_batch = self._to_tensor(batch.terminal).transpose(0,1).unsqueeze(2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            input_dict = {'obs':next_state_batch, 'prev_state': next_hidden_state}\n",
    "            Q_next_1 = torch.min(self.model.q_target_1(input_dict)['logit'], dim=2)[0].unsqueeze(2)\n",
    "            Q_next_2 = torch.min(self.model.q_target_2(input_dict)['logit'], dim=2)[0].unsqueeze(2)\n",
    "            # Use max want to avoid underestimation bias\n",
    "            Q_next = torch.max(Q_next_1, Q_next_2)\n",
    "            Q_expected = reward_batch + self.gamma * Q_next * (1 - terminal_batch)\n",
    "\n",
    "        Q_1 = self.model.q_1(\n",
    "            {'obs':state_batch, 'prev_state': hidden_state})\n",
    "        Q_2 = self.model.q_2(\n",
    "            {'obs':state_batch, 'prev_state': hidden_state})\n",
    "\n",
    "        Q_1 = Q_1['logit'].gather(2, action_batch)\n",
    "        Q_2 = Q_2['logit'].gather(2, action_batch)\n",
    "        L_1 = nn.MSELoss()(Q_1, Q_expected)\n",
    "        L_2 = nn.MSELoss()(Q_2, Q_expected)\n",
    "\n",
    "        self.loss.append([L_1.item(), L_2.item()])\n",
    "        self.model.q_optimizer_1.zero_grad()\n",
    "        self.model.q_optimizer_2.zero_grad()\n",
    "        L_1.backward()\n",
    "        L_2.backward()\n",
    "        self.model.q_optimizer_1.step()\n",
    "        self.model.q_optimizer_2.step()\n",
    "\n",
    "        if (self.training_iter % self.update_freq) == 0:\n",
    "            self.model.update_target_nn()\n",
    "        self.model.grad_update_num +=1\n",
    "\n",
    "\n",
    "    def train(self, num_decisions=100): ###\n",
    "        \"\"\"Train q networks\n",
    "        Args:\n",
    "            num_decisions: Number of decisions to train algorithm for\n",
    "        \"\"\"\n",
    "        # self.folder_name = \"./Results\" + str(sweep_var)\n",
    "        # os.system(\"mkdir \" + self.folder_name)\n",
    "        self.update_plot = DynamicUpdate(self.delta_t,self.folder_name)\n",
    "\n",
    "        episodes = 350 ###\n",
    "        e = np.arange(episodes)\n",
    "        T = 300 # choosing how fast to move from exploration to exploitation\n",
    "        eps = -np.log10(e/T)\n",
    "        self.epsilon = np.clip(eps,0.05,1) # clipping to ensure all values are between 0 and 1\n",
    "\n",
    "        for i in range(self.episode_num,episodes):\n",
    "            print(\"Episode: \", i)\n",
    "\n",
    "            # warmup\n",
    "            b, hidden_state = self.init\n",
    "            state = [0]*self.embed_multiplier\n",
    "            self.sim_controller.initialize(b)\n",
    "            _, cell_count = self.sim_controller.simulate_population(self.sim_controller.num_cells_init, b)\n",
    "            for k in range(1,36+1):\n",
    "                _, cell_count = self.sim_controller.simulate_population(cell_count[-1], b)\n",
    "                if k == 36:\n",
    "                    _, hidden_state = self._get_action(state, hidden_state, episode=i)\n",
    "                    state, _, _ = self.get_state_reward(state, cell_count, b)\n",
    "\n",
    "            state_seq = []\n",
    "            action_seq = []\n",
    "            reward_seq = []\n",
    "            next_state_seq = []\n",
    "            terminal_seq = []\n",
    "            self.it = 1\n",
    "\n",
    "            for j in range(num_decisions):\n",
    "                state_seq.append(copy.deepcopy(state))\n",
    "                if self.it == 1:\n",
    "                    hidden_state_init = hidden_state\n",
    "                action_index, hidden_state = self._get_action(state, hidden_state, episode=i)\n",
    "                if self.it == 1:\n",
    "                    next_hidden_state_init = hidden_state\n",
    "                action_seq.append(action_index)\n",
    "\n",
    "                action_b = self.b_actions[self.b_index[action_index]]\n",
    "\n",
    "                _, cell_count = self.sim_controller.simulate_population(cell_count[-1], action_b)\n",
    "                state, reward, terminal = self.get_state_reward(state, cell_count, action_b/max(self.b_actions))\n",
    "\n",
    "                reward_seq.append(reward)\n",
    "                next_state_seq.append(copy.deepcopy(state))\n",
    "                terminal_seq.append(terminal)\n",
    "\n",
    "                if self.it < self.max_seq_len:\n",
    "                    self.it +=1\n",
    "                else:\n",
    "                    transition_to_add = [state_seq, action_seq, reward_seq, next_state_seq, hidden_state_init,\n",
    "                        next_hidden_state_init, terminal_seq]\n",
    "                    self.it = 1\n",
    "                    state_seq = []\n",
    "                    action_seq = []\n",
    "                    reward_seq = []\n",
    "                    next_state_seq = []\n",
    "                    terminal_seq = []\n",
    "\n",
    "                    self.buffer.push(*list(transition_to_add))\n",
    "                    if cell_count[-1] == 0 or cell_count[-1] > self.max_pop:\n",
    "                        self._update()\n",
    "                        break\n",
    "                self._update()\n",
    "\n",
    "            if (i % 10 == 0) or (i == episodes-1):\n",
    "                self.eval(i)\n",
    "                self._save_data(i)\n",
    "                if i == episodes-1:\n",
    "                    plot_reward_Q_loss(self.ave_sum_rewards, self.std_sum_rewards, self.grad_updates, self.loss, self.update_plot.folder_name_test,\n",
    "                                   self.ave_Q1, self.ave_Q2, self.ave_Q1_target, self.ave_Q2_target)\n",
    "\n",
    "\n",
    "\n",
    "    def eval(self, episode, num_decisions=5, num_evals=5): ###\n",
    "        \"\"\"Given trained q networks, generate trajectories\n",
    "        \"\"\"\n",
    "        print(\"Evaluation\")\n",
    "\n",
    "        extinct_times = []\n",
    "        extinct_count = 0\n",
    "        max_cross_corr_kn0 = []\n",
    "        max_cross_corr_U = []\n",
    "        lag_kn0 = []\n",
    "        lag_U = []\n",
    "\n",
    "        # results = Parallel(n_jobs=5)(delayed(self.rollout)(num_decisions) for i in range(num_evals))\n",
    "        results = [self.rollout(num_decisions) for i in range(num_evals)]\n",
    "\n",
    "        b, cell_count, t, kn0, sum_rewards, min_Q1, min_Q1_target, min_Q2, min_Q2_target, U = list(zip(*results))\n",
    "        for drug, nutr, damage, time, count in zip(b,kn0,U,t,cell_count):\n",
    "\n",
    "            # compute max cross correlation and lag\n",
    "            if np.std(drug[1:]) > 0:\n",
    "                cross_correlation(drug[1:], nutr[1:], max_cross_corr_kn0, lag_kn0)\n",
    "                cross_correlation(drug[1:], damage[1:], max_cross_corr_U, lag_U)\n",
    "\n",
    "            # save extinction times\n",
    "            if count[-1] == 0:\n",
    "                    extinct_times.append(time[-1])\n",
    "                    extinct_count +=1\n",
    "\n",
    "        self.ave_sum_rewards.append(np.array(sum_rewards).mean())\n",
    "        self.std_sum_rewards.append(np.array(sum_rewards).std())\n",
    "        self.ave_Q1.append(np.array(min_Q1).mean())\n",
    "        self.ave_Q2.append(np.array(min_Q2).mean())\n",
    "        self.ave_Q1_target.append(np.array(min_Q1_target).mean())\n",
    "        self.ave_Q2_target.append(np.array(min_Q2_target).mean())\n",
    "        self.grad_updates.append(self.model.grad_update_num)\n",
    "\n",
    "        # save results\n",
    "        ave_ext_time = sum(extinct_times)/len(extinct_times) if len(extinct_times) > 0 else np.Inf\n",
    "        ave_max_cross_corr_kn0 = sum(max_cross_corr_kn0)/len(max_cross_corr_kn0) if len(max_cross_corr_kn0) > 0 else 0\n",
    "        ave_corr_lag_kn0 = sum(lag_kn0)/len(lag_kn0) if len(lag_kn0) > 0 else 0\n",
    "        ave_max_cross_corr_U = sum(max_cross_corr_U)/len(max_cross_corr_U) if len(max_cross_corr_U) > 0 else 0\n",
    "        ave_corr_lag_U = sum(lag_U)/len(lag_U) if len(lag_U) > 0 else 0\n",
    "        # log via wandb\n",
    "        # wandb.log({\"extinct_fraction\": extinct_count/num_evals,\n",
    "        #     \"ave_ext_rate\": 1/ave_ext_time,\n",
    "        #     \"ave_max_cross_corr_kn0\": ave_max_cross_corr_kn0,\n",
    "        #     \"ave_corr_lag_kn0\": ave_corr_lag_kn0,\n",
    "        #     \"ave_max_cross_corr_U\": ave_max_cross_corr_U,\n",
    "        #     \"ave_corr_lag_U\": ave_corr_lag_U,\n",
    "        #     \"ave total reward\": np.array(sum_rewards).mean(),\n",
    "        #     \"ave min Q1\": np.array(min_Q1).mean()})\n",
    "\n",
    "        # select trajectories randomly to plot\n",
    "        rand_i = random.sample(range(num_evals), 5)\n",
    "        self.update_plot(episode, t, cell_count, kn0, b, rand_i)\n",
    "\n",
    "\n",
    "    def rollout(self, num_decisions):\n",
    "        b_all = np.zeros((num_decisions+1,1))\n",
    "        cell_count_all = np.zeros((num_decisions+1,1))\n",
    "        t_all = np.zeros((num_decisions+1,1))\n",
    "        kn0_all = np.zeros((num_decisions+1,1))\n",
    "        rewards_all = np.zeros((num_decisions+1,1))\n",
    "        Q1_all = np.zeros((num_decisions+1,1))\n",
    "        Q1_target_all = np.zeros((num_decisions+1,1))\n",
    "        Q2_all = np.zeros((num_decisions+1,1))\n",
    "        Q2_target_all = np.zeros((num_decisions+1,1))\n",
    "        Uave_all = np.zeros((num_decisions+1,1))\n",
    "\n",
    "        # warmup\n",
    "        b, hidden_state = self.init\n",
    "        state = [0]*self.embed_multiplier\n",
    "        self.sim_controller.initialize(b)\n",
    "        _, cell_count = self.sim_controller.simulate_population(self.sim_controller.num_cells_init, b)\n",
    "        for k in range(1,36+1):\n",
    "            t, cell_count = self.sim_controller.simulate_population(cell_count[-1], b)\n",
    "            if k == 36:\n",
    "                _, hidden_state, Q1_all[k-36], Q1_target_all[k-36], Q2_all[k-36], Q2_target_all[k-36] = self._get_action(state, hidden_state, eval=True) # calling to generate hidden state and to save Q value for plot\n",
    "                state, rewards_all[k-36], _ = self.get_state_reward(state, cell_count, b)\n",
    "\n",
    "                b_all[k-36] = b\n",
    "                cell_count_all[k-36] = cell_count[-1]\n",
    "                t_all[k-36] = t[-1]\n",
    "                kn0_all[k-36] = self.sim_controller.k_n0\n",
    "                Uave_all[k-36] = self.sim_controller.U_ave\n",
    "\n",
    "        for j in range(num_decisions):\n",
    "            action_index, hidden_state, Q1_all[j+1], Q1_target_all[j+1], Q2_all[j+1], Q2_target_all[j+1] = self._get_action(state, hidden_state, eval=True)\n",
    "            action_b = self.b_actions[self.b_index[action_index]]\n",
    "\n",
    "            t, cell_count = self.sim_controller.simulate_population(cell_count[-1], action_b)\n",
    "            state, rewards_all[j+1], _ = self.get_state_reward(state, cell_count, action_b/max(self.b_actions))\n",
    "\n",
    "            b_all[j+1] = action_b\n",
    "            cell_count_all[j+1] = cell_count[-1]\n",
    "            t_all[j+1] = t[-1]\n",
    "            kn0_all[j+1] = self.sim_controller.k_n0\n",
    "            Uave_all[j+1] = self.sim_controller.U_ave\n",
    "            if cell_count[-1] == 0 or cell_count[-1] > self.max_pop:\n",
    "                # trim arrays to length of episode\n",
    "                b_all, cell_count_all, t_all, kn0_all, Uave_all = trim([b_all,cell_count_all,t_all,kn0_all,Uave_all], j+1+1)\n",
    "                break\n",
    "\n",
    "        sum_rewards = np.array([rewards_all.sum()])\n",
    "        return b_all, cell_count_all, t_all, kn0_all, sum_rewards, Q1_all, Q1_target_all, Q2_all, Q2_target_all, Uave_all\n",
    "\n",
    "\n",
    "def trim(list, trim_ind):\n",
    "    return [vec[:trim_ind] for vec in list]\n",
    "\n",
    "def cross_correlation(sig1, sig2, max_cross_corr, lag):\n",
    "    n_points = sig1.size\n",
    "    cross_corr = signal.correlate(sig1 - np.mean(sig1), sig2 - np.mean(sig2), mode='full')\n",
    "    cross_corr /= (np.std(sig1) * np.std(sig2) * n_points)  # Normalize\n",
    "    max_cross_corr.append(np.max(cross_corr))\n",
    "    lags = signal.correlation_lags(sig1.size,sig2.size, mode=\"full\")\n",
    "    lag.append(lags[np.argmax(cross_corr)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: ./Results/Eval: File exists\n",
      "/var/folders/_2/b28clpbs6t36hp6grpxntpq40000gn/T/ipykernel_97182/1121750218.py:217: RuntimeWarning: divide by zero encountered in log10\n",
      "  eps = -np.log10(e/T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0\n",
      "Evaluation\n",
      "Episode:  1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m c \u001b[38;5;241m=\u001b[39m CDQL(training_config\u001b[38;5;241m=\u001b[39mtraining_config)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# c.load_data()\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 229\u001b[0m, in \u001b[0;36mCDQL.train\u001b[0;34m(self, num_decisions)\u001b[0m\n\u001b[1;32m    227\u001b[0m _, cell_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim_controller\u001b[38;5;241m.\u001b[39msimulate_population(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim_controller\u001b[38;5;241m.\u001b[39mnum_cells_init, b)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m36\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 229\u001b[0m     _, cell_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msim_controller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulate_population\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell_count\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m36\u001b[39m:\n\u001b[1;32m    231\u001b[0m         _, hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_action(state, hidden_state, episode\u001b[38;5;241m=\u001b[39mi)\n",
      "File \u001b[0;32m~/Documents/JUPYTER NOTEBOOK/RL-for-bacterial-control/cell_model_full_parallel.py:249\u001b[0m, in \u001b[0;36mCell_Population.simulate_population\u001b[0;34m(self, true_num_cells, b, k_n0, n_steps, threshold)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    248\u001b[0m     kn0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_n0\n\u001b[0;32m--> 249\u001b[0m species_stack \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultiIntegrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspecies_stack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkn0\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# integrating one timestep\u001b[39;00m\n\u001b[1;32m    250\u001b[0m kn0 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdkn0dt(t[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], kn0)\u001b[38;5;241m*\u001b[39mdt \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigma_kn0)\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39msqrt(dt)\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal()\n\u001b[1;32m    251\u001b[0m kn0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(kn0, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m5.0\u001b[39m) \u001b[38;5;66;03m# clipping values to keep in physiological range\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/JUPYTER NOTEBOOK/RL-for-bacterial-control/cell_model_full_parallel.py:128\u001b[0m, in \u001b[0;36mCell_Population.MultiIntegrate\u001b[0;34m(self, Species, t, dt, b, k_n0)\u001b[0m\n\u001b[1;32m    125\u001b[0m     U[:] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    126\u001b[0m U[U \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 128\u001b[0m X \u001b[38;5;241m=\u001b[39m X_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdXdt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphiR_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mU_i\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m*\u001b[39mdt\n\u001b[1;32m    129\u001b[0m V \u001b[38;5;241m=\u001b[39m V_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdVdt(V_i, t, a_i, phiR_i, U_i)\u001b[38;5;241m*\u001b[39mdt\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([phi_R,phi_S,a,U,X,V])\n",
      "File \u001b[0;32m~/Documents/JUPYTER NOTEBOOK/RL-for-bacterial-control/cell_model_full_parallel.py:93\u001b[0m, in \u001b[0;36mCell_Population.dXdt\u001b[0;34m(self, X, t, a, phi_R, V, U)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdXdt\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, t, a, phi_R, V, U):\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# division protein ODE\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     dxdt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_X\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mU\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGrowthRate(a, phi_R, U) \u001b[38;5;241m*\u001b[39m V \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmu \u001b[38;5;241m*\u001b[39m X\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dxdt\n",
      "File \u001b[0;32m~/Documents/JUPYTER NOTEBOOK/RL-for-bacterial-control/cell_model_full_parallel.py:55\u001b[0m, in \u001b[0;36mCell_Population.f_X\u001b[0;34m(self, a, U)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf_X\u001b[39m(\u001b[38;5;28mself\u001b[39m, a, U):\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphaX\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphiR_max \u001b[38;5;241m-\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_R\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mU\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbetaX\n",
      "File \u001b[0;32m~/Documents/JUPYTER NOTEBOOK/RL-for-bacterial-control/cell_model_full_parallel.py:52\u001b[0m, in \u001b[0;36mCell_Population.f_R\u001b[0;34m(self, a, U)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf_R\u001b[39m(\u001b[38;5;28mself\u001b[39m, a, U):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_prime(a)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mg\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphiR_max\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_S(U)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf(a)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_prime(a)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphiR_min) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_prime(a)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg(a) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf(a)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_prime(a))\n",
      "File \u001b[0;32m~/Documents/JUPYTER NOTEBOOK/RL-for-bacterial-control/cell_model_full_parallel.py:42\u001b[0m, in \u001b[0;36mCell_Population.g\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf_prime\u001b[39m(\u001b[38;5;28mself\u001b[39m, a):\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_f\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma_n)\u001b[38;5;241m*\u001b[39m(a\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma_n)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_f\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m (a\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma_n)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_f)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;66;03m# derivative of f w.r.t. a\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mg\u001b[39m(\u001b[38;5;28mself\u001b[39m, a):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma_t)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_g \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m (a\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma_t)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_g) \u001b[38;5;66;03m# regulatory function for k_t\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mg_prime\u001b[39m(\u001b[38;5;28mself\u001b[39m, a):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_config = {}\n",
    "training_config[\"T\"] = 12\n",
    "training_config[\"max_seq_len\"] = 4\n",
    "training_config[\"folder_name\"] = \"./Results\"\n",
    "c = CDQL(training_config=training_config)\n",
    "# c.load_data()\n",
    "c.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
