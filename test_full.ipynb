{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from cell_model_full_parallel import Cell_Population\n",
    "from replaybuffer import ReplayBuffer\n",
    "from deepQLnetwork import Model\n",
    "from joblib import Parallel, delayed\n",
    "from utils_figure_plot import DynamicUpdate, plot_reward_Q_loss\n",
    "import copy\n",
    "from scipy import signal\n",
    "import matplotlib as mpl\n",
    "# import wandb\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "\n",
    "class CDQL:\n",
    "\n",
    "    # later add in learning rate and tau (update rate) as arguments to tune over as well\n",
    "    #  - although learning rate and batch size have similar effects\n",
    "\n",
    "    def __init__(self, delay_embed_len=10,\n",
    "            batch_size=512,\n",
    "            delta_t=0.2,\n",
    "            omega=0.02,\n",
    "            gamma=0.99,\n",
    "            update_freq=2,\n",
    "            use_gpu=False,\n",
    "            num_cells_init=60,\n",
    "            kn0_mean=2.55,\n",
    "            T=12,\n",
    "            agent_input=\"full\"):\n",
    "\n",
    "        self.gamma = gamma\n",
    "        if use_gpu and torch.cuda.is_available(): # and torch.cuda.device_count() > 1:\n",
    "            self.device = torch.device('cuda')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "        assert (not use_gpu) or (self.device == torch.device('cuda'))\n",
    "\n",
    "        self.delta_t = delta_t\n",
    "        self.sim_controller = Cell_Population(num_cells_init, delta_t, omega, kn0_mean, T)\n",
    "        self.buffer = ReplayBuffer(1e6)\n",
    "        self.delay_embed_len = delay_embed_len\n",
    "        self.batch_size = batch_size\n",
    "        self.b_actions = [0.0, 3.72]\n",
    "        self.b_num_actions = len(self.b_actions)\n",
    "        self.num_actions = self.b_num_actions\n",
    "        self.b_index = [0, 1]\n",
    "\n",
    "        if agent_input == \"full\":\n",
    "            self.get_state_reward = self.sim_controller.get_reward_all\n",
    "            self.embed_multiplier = 3\n",
    "        elif agent_input == \"no_nutrient\":\n",
    "            self.get_state_reward = self.sim_controller.get_reward_no_nutrient\n",
    "            self.embed_multiplier = 2\n",
    "        elif agent_input == \"no_act\":\n",
    "            self.get_state_reward = self.sim_controller.get_reward_no_antibiotic\n",
    "            self.embed_multiplier = 2\n",
    "        else:\n",
    "            raise ValueError(\"Invalid agent_input value.\")\n",
    "\n",
    "        self.model = Model(self.device, num_inputs=self.delay_embed_len*self.embed_multiplier, num_actions=self.num_actions)\n",
    "\n",
    "        self.init = 0.0 # b_init\n",
    "        self.loss = []\n",
    "        self.ave_sum_rewards = []\n",
    "        self.std_sum_rewards = []\n",
    "        self.ave_Q1 = []\n",
    "        self.ave_Q2 = []\n",
    "        self.ave_Q1_target = []\n",
    "        self.ave_Q2_target = []\n",
    "        self.grad_updates = []\n",
    "        self.training_iter = 0\n",
    "        self.update_freq = update_freq\n",
    "        self.epsilon = None\n",
    "        self.episode_num = 0\n",
    "\n",
    "    def _to_tensor(self, x):\n",
    "        return torch.tensor(x).float().to(self.device)\n",
    "\n",
    "    def _save_data(self, episode_num):\n",
    "        np.save(self.folder_name + \"/replaybuffer\", np.array(self.buffer.buffer, dtype=object))\n",
    "\n",
    "        self.model.save_networks(self.folder_name+'/')\n",
    "        self.save_episode_num(episode_num)\n",
    "\n",
    "    def save_episode_num(self, episode_num, filename='/episode_num.txt'):\n",
    "        filename = self.folder_name + filename\n",
    "        with open(filename,\"w\") as file:\n",
    "            file.write(str(episode_num))\n",
    "\n",
    "    def load_data(self, sweep_var, folder_name=\"./Results\"):\n",
    "        self.folder_name = folder_name + str(sweep_var)\n",
    "        self.buffer.load_buffer(self.folder_name + \"/replaybuffer.npy\")\n",
    "        self.model.load_networks(self.folder_name)\n",
    "        self.episode_num = self.load_episode_num()\n",
    "\n",
    "    def load_episode_num(self, filename='/episode_num.txt'):\n",
    "        filename = self.folder_name + filename\n",
    "        try:\n",
    "            with open(filename, \"r\") as file:\n",
    "                episode_num = int(file.read().strip()) + 1\n",
    "            print(\"Partially trained model found, starting from episode \",episode_num,\".\")\n",
    "            return episode_num\n",
    "        except FileNotFoundError:\n",
    "            return 0\n",
    "\n",
    "    def _get_action(self, state, episode=None, eval=False):\n",
    "        \"\"\"Gets action given some state using epsilon-greedy algorithm\n",
    "        Args:\n",
    "            state: List defining the state\n",
    "            episode: episode number\n",
    "        \"\"\"\n",
    "\n",
    "        if not eval:\n",
    "            explore = random.random()\n",
    "            if (explore < self.epsilon[episode]):\n",
    "                action = random.choice(list(range(self.num_actions)))\n",
    "                return action\n",
    "\n",
    "        self.model.q_1.eval()\n",
    "        with torch.no_grad():\n",
    "            curr_state = self._to_tensor(state)\n",
    "            curr_state = curr_state.unsqueeze(0)\n",
    "            action = torch.argmin(\n",
    "                    self.model.q_1(curr_state), dim=1).item()\n",
    "            if eval:\n",
    "                store_Q1 = [self.model.q_1(curr_state).squeeze(0)[i].item() for i in range(self.num_actions)]\n",
    "                store_Q_target1 = [self.model.q_target_1(curr_state).squeeze(0)[i].item() for i in range(self.num_actions)]\n",
    "                store_Q2 = [self.model.q_2(curr_state).squeeze(0)[i].item() for i in range(self.num_actions)]\n",
    "                store_Q_target2 = [self.model.q_target_2(curr_state).squeeze(0)[i].item() for i in range(self.num_actions)]\n",
    "        self.model.q_1.train()\n",
    "\n",
    "        return (action, store_Q1, store_Q_target1, store_Q2, store_Q_target2) if eval else action\n",
    "\n",
    "    def _update(self):\n",
    "        \"\"\"Updates q1, q2, q1_target and q2_target networks based on clipped Double Q Learning Algorithm\n",
    "        \"\"\"\n",
    "        if (len(self.buffer) < self.batch_size):\n",
    "            return\n",
    "        self.training_iter += 1\n",
    "        # Make sure actor_target and critic_target are in eval mode\n",
    "        assert not self.model.q_target_1.training\n",
    "        assert not self.model.q_target_2.training\n",
    "\n",
    "        assert self.model.q_1.training\n",
    "        assert self.model.q_2.training\n",
    "        transitions = self.buffer.sample(self.batch_size)\n",
    "        batch = self.buffer.transition(*zip(*transitions))\n",
    "        state_batch = self._to_tensor(batch.state)\n",
    "        action_batch = self._to_tensor(\n",
    "            batch.action).unsqueeze(1).to(torch.int64)\n",
    "        reward_batch = self._to_tensor(batch.reward)\n",
    "        next_state_batch = self._to_tensor(batch.next_state)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Add noise to smooth out learning\n",
    "            Q_next_1 = torch.min(self.model.q_target_1(\n",
    "                next_state_batch), dim=1)[0].unsqueeze(1)\n",
    "            Q_next_2 = torch.min(self.model.q_target_2(\n",
    "                next_state_batch), dim=1)[0].unsqueeze(1)\n",
    "            # Use max want to avoid underestimation bias #####\n",
    "            Q_next = torch.max(Q_next_1, Q_next_2)\n",
    "            Q_expected = reward_batch + self.gamma * Q_next  # No \"Terminal State\"\n",
    "\n",
    "        Q_1 = self.model.q_1(state_batch).gather(1, action_batch)\n",
    "        Q_2 = self.model.q_2(state_batch).gather(1, action_batch)\n",
    "        L_1 = nn.MSELoss()(Q_1, Q_expected)\n",
    "        L_2 = nn.MSELoss()(Q_2, Q_expected)\n",
    "\n",
    "        self.loss.append([L_1.item(), L_2.item()])\n",
    "        self.model.q_optimizer_1.zero_grad()\n",
    "        self.model.q_optimizer_2.zero_grad()\n",
    "        L_1.backward()\n",
    "        L_2.backward()\n",
    "        self.model.q_optimizer_1.step()\n",
    "        self.model.q_optimizer_2.step()\n",
    "\n",
    "        # Q_1_new = self.model.q_1(state_batch).gather(1, action_batch)\n",
    "        # print(\"  %.2f\"%Q_expected.min().item())\n",
    "        # print(\"    %.2f\"%(Q_1_new - Q_1).min().item())\n",
    "        # self.store_Q.append([Q_1.tolist(), Q_2.tolist(), Q_expected.tolist()])\n",
    "\n",
    "        if (self.training_iter % self.update_freq) == 0:\n",
    "            self.model.update_target_nn()\n",
    "        self.model.grad_update_num +=1\n",
    "\n",
    "\n",
    "    def train(self, sweep_var, num_decisions=5): #200\n",
    "        \"\"\"Train q networks\n",
    "        Args:\n",
    "            num_decisions: Number of decisions to train algorithm for\n",
    "        \"\"\"\n",
    "        self.folder_name = \"./Results\" + str(sweep_var)\n",
    "        os.system(\"mkdir \" + self.folder_name)\n",
    "        self.update_plot = DynamicUpdate(self.delta_t,self.delay_embed_len,self.folder_name)\n",
    "\n",
    "        episodes = 350 #400\n",
    "        e = np.arange(episodes)\n",
    "        T = 300 # 380, choosing how fast to move from exploration to exploitation\n",
    "        eps = -np.log10(e/T)\n",
    "        self.epsilon = np.clip(eps,0.05,1) # clipping to ensure all values are between 0 and 1\n",
    "\n",
    "        for i in range(self.episode_num,episodes):\n",
    "            print(\"Episode: \", i)\n",
    "            # episode_folder_name = self.folder_name + \"Train/\" + str(i) + \"/\"\n",
    "\n",
    "            # os.system(\"mkdir -p \" + episode_folder_name)\n",
    "            # filename = episode_folder_name + str(i)\n",
    "\n",
    "            # warmup\n",
    "            b = self.init\n",
    "            state = [0]*self.delay_embed_len*self.embed_multiplier\n",
    "            self.sim_controller.initialize(b)\n",
    "            _, cell_count = self.sim_controller.simulate_population(self.sim_controller.num_cells_init, b)\n",
    "            for k in range(1,36+self.delay_embed_len):\n",
    "                _, cell_count = self.sim_controller.simulate_population(cell_count[-1], b)\n",
    "                if k >= 36:\n",
    "                    state, _ = self.get_state_reward(state, cell_count, b)\n",
    "\n",
    "            for j in range(num_decisions):\n",
    "                action_index = self._get_action(state, i)\n",
    "                transition_to_add = [copy.deepcopy(state), action_index]\n",
    "\n",
    "                action_b = self.b_actions[self.b_index[action_index]]\n",
    "\n",
    "                _, cell_count = self.sim_controller.simulate_population(cell_count[-1], action_b)\n",
    "                state, reward = self.get_state_reward(state, cell_count, action_b/max(self.b_actions))\n",
    "                transition_to_add.extend([[reward], copy.deepcopy(state)])\n",
    "                self.buffer.push(*list(transition_to_add))\n",
    "                self._update()\n",
    "                if cell_count[-1] == 0:\n",
    "                    break\n",
    "\n",
    "            if (i % 10 == 0) or (i == episodes-1):\n",
    "                self.eval(i)\n",
    "                self._save_data(i)\n",
    "                if i == episodes-1:\n",
    "                    plot_reward_Q_loss(self.ave_sum_rewards, self.std_sum_rewards, self.grad_updates, self.loss, self.update_plot.folder_name_test,\n",
    "                                   self.ave_Q1, self.ave_Q2, self.ave_Q1_target, self.ave_Q2_target)\n",
    "\n",
    "\n",
    "\n",
    "    def eval(self, episode, num_decisions=5, num_evals=5): #200, 50\n",
    "        \"\"\"Given trained q networks, generate trajectories\n",
    "        \"\"\"\n",
    "        print(\"Evaluation\")\n",
    "\n",
    "        extinct_times = []\n",
    "        extinct_count = 0\n",
    "        max_cross_corr = []\n",
    "        lag = []\n",
    "\n",
    "        results = Parallel(n_jobs=-1)(delayed(self.rollout)(num_decisions) for i in range(num_evals))\n",
    "        # results = [self.rollout(num_decisions) for i in range(num_evals)]\n",
    "\n",
    "        i=0\n",
    "        for r in results:\n",
    "            if i == 0:\n",
    "                b_all, cell_count_all, t_all, kn0_all, rewards_all, Q1, Q1_target, Q2, Q2_target = r\n",
    "                sum_reward = np.array([rewards_all.sum()])\n",
    "                min_Q1 = np.array(Q1.min(axis=1))\n",
    "                min_Q1_target = np.array(Q1_target.min(axis=1))\n",
    "                min_Q2 = np.array(Q2.min(axis=1))\n",
    "                min_Q2_target = np.array(Q2_target.min(axis=1))\n",
    "                if not np.all(cell_count_all > 0):\n",
    "                    ind = np.where(cell_count_all == 0)[0][0]\n",
    "                    extinct_times.append(t_all[ind])\n",
    "                    extinct_count +=1\n",
    "                    ind +=1\n",
    "                else:\n",
    "                    ind = b_all.size\n",
    "                # compute max cross correlation and lag\n",
    "                if np.std(b_all[:ind]) > 0:\n",
    "                    n_points = ind\n",
    "                    cross_corr = signal.correlate(b_all[:ind] - np.mean(b_all[:ind]), kn0_all[:ind] - np.mean(kn0_all[:ind]), mode='full')\n",
    "                    cross_corr /= (np.std(b_all[:ind]) * np.std(kn0_all[:ind]) * n_points)  # Normalize\n",
    "                    max_cross_corr.append(np.max(cross_corr))\n",
    "                    lags = signal.correlation_lags(b_all[:ind].size,kn0_all[:ind].size, mode=\"full\")\n",
    "                    lag.append(lags[np.argmax(cross_corr)])\n",
    "                i+=1\n",
    "            else:\n",
    "                b, cell_count, t, kn0, rewards, Q1, Q1_target, Q2, Q2_target = r\n",
    "                b_all = np.concatenate((b_all, b), axis=1)\n",
    "                cell_count_all = np.concatenate((cell_count_all, cell_count), axis=1)\n",
    "                t_all = np.concatenate((t_all,t), axis=1)\n",
    "                kn0_all = np.concatenate((kn0_all,kn0), axis=1)\n",
    "                rewards_all = np.concatenate((rewards_all,rewards), axis=1)\n",
    "                sum_reward = np.concatenate((sum_reward, np.array([rewards.sum()])))\n",
    "                min_Q1 = np.vstack((min_Q1, np.array(Q1.min(axis=1))))\n",
    "                min_Q1_target = np.vstack((min_Q1_target, np.array(Q1_target.min(axis=1))))\n",
    "                min_Q2 = np.vstack((min_Q2, np.array(Q2.min(axis=1))))\n",
    "                min_Q2_target = np.vstack((min_Q2_target, np.array(Q2_target.min(axis=1))))\n",
    "                if not np.all(cell_count > 0):\n",
    "                    ind = np.where(cell_count == 0)[0][0]\n",
    "                    extinct_times.append(t[ind])\n",
    "                    extinct_count +=1\n",
    "                    ind +=1\n",
    "                else:\n",
    "                    ind = b.size\n",
    "                # compute max cross correlation and lag\n",
    "                if np.std(b[:ind]) > 0:\n",
    "                    n_points = ind\n",
    "                    cross_corr = signal.correlate(b[:ind] - np.mean(b[:ind]), kn0[:ind] - np.mean(kn0[:ind]), mode='full')\n",
    "                    cross_corr /= (np.std(b[:ind]) * np.std(kn0[:ind]) * n_points)  # Normalize\n",
    "                    max_cross_corr.append(np.max(cross_corr))\n",
    "                    lags = signal.correlation_lags(b[:ind].size,kn0[:ind].size, mode=\"full\")\n",
    "                    lag.append(lags[np.argmax(cross_corr)])\n",
    "\n",
    "        self.ave_sum_rewards.append(sum_reward.mean())\n",
    "        self.std_sum_rewards.append(sum_reward.std())\n",
    "        self.ave_Q1.append(min_Q1.mean())\n",
    "        self.ave_Q2.append(min_Q2.mean())\n",
    "        self.ave_Q1_target.append(min_Q1_target.mean())\n",
    "        self.ave_Q2_target.append(min_Q2_target.mean())\n",
    "        self.grad_updates.append(self.model.grad_update_num)\n",
    "        # save extinction quantification results\n",
    "        if len(extinct_times) > 0:\n",
    "            ave_ext_time = sum(extinct_times)/len(extinct_times)\n",
    "        else:\n",
    "            ave_ext_time = np.Inf\n",
    "\n",
    "        # log via wandb\n",
    "        # wandb.log({\"extinct_fraction\": extinct_count/num_evals,\n",
    "        #     \"ave_ext_rate\": 1/ave_ext_time,\n",
    "        #     \"ave_max_cross_corr\": sum(max_cross_corr)/len(max_cross_corr),\n",
    "        #     \"ave_corr_lag\": sum(lag)/len(lag),\n",
    "        #     \"ave total reward\": sum_reward.mean(),\n",
    "        #     \"ave min Q1\": min_Q1.mean()})\n",
    "        # select five trajectories randomly to plot\n",
    "        rand_i = random.sample(range(num_evals), 5)\n",
    "        self.update_plot(episode, t_all[:,rand_i], cell_count_all[:,rand_i], kn0_all[:,rand_i], b_all[:,rand_i])\n",
    "\n",
    "\n",
    "    def rollout(self, num_decisions):\n",
    "        b_all = np.zeros((num_decisions+self.delay_embed_len,1))\n",
    "        cell_count_all = np.zeros((num_decisions+self.delay_embed_len,1))\n",
    "        t_all = np.zeros((num_decisions+self.delay_embed_len,1))\n",
    "        kn0_all = np.zeros((num_decisions+self.delay_embed_len,1))\n",
    "        rewards_all = np.zeros((num_decisions+self.delay_embed_len,1))\n",
    "        Q1_all = np.zeros((num_decisions+self.delay_embed_len,2))\n",
    "        Q1_target_all = np.zeros((num_decisions+self.delay_embed_len,2))\n",
    "        Q2_all = np.zeros((num_decisions+self.delay_embed_len,2))\n",
    "        Q2_target_all = np.zeros((num_decisions+self.delay_embed_len,2))\n",
    "\n",
    "        # warmup\n",
    "        b = self.init\n",
    "        state = [0]*self.delay_embed_len*self.embed_multiplier\n",
    "        self.sim_controller.initialize(b)\n",
    "        _, cell_count = self.sim_controller.simulate_population(self.sim_controller.num_cells_init, b)\n",
    "        for k in range(1,36+self.delay_embed_len):\n",
    "            t, cell_count = self.sim_controller.simulate_population(cell_count[-1], b)\n",
    "            if k >= 36:\n",
    "                _, Q1_all[k-36,:], Q1_target_all[k-36,:], Q2_all[k-36,:], Q2_target_all[k-36,:] = self._get_action(state, eval=True) # just calling this to save Q value for plot\n",
    "                state, rewards_all[k-36] = self.get_state_reward(state, cell_count, b)\n",
    "\n",
    "                b_all[k-36] = b\n",
    "                cell_count_all[k-36] = cell_count[-1]\n",
    "                t_all[k-36] = t[-1]\n",
    "                kn0_all[k-36] = self.sim_controller.k_n0\n",
    "\n",
    "        for j in range(num_decisions):\n",
    "            action_index, Q1_all[j+self.delay_embed_len,:], Q1_target_all[j+self.delay_embed_len,:], Q2_all[j+self.delay_embed_len,:], Q2_target_all[j+self.delay_embed_len,:] = self._get_action(state, eval=True)\n",
    "            action_b = self.b_actions[self.b_index[action_index]]\n",
    "\n",
    "            t, cell_count = self.sim_controller.simulate_population(cell_count[-1], action_b)\n",
    "            state, rewards_all[j+self.delay_embed_len] = self.get_state_reward(state, cell_count, action_b/max(self.b_actions))\n",
    "\n",
    "            b_all[j+self.delay_embed_len] = action_b\n",
    "            cell_count_all[j+self.delay_embed_len] = cell_count[-1]\n",
    "            t_all[j+self.delay_embed_len] = t[-1]\n",
    "            kn0_all[j+self.delay_embed_len] = self.sim_controller.k_n0\n",
    "            if cell_count[-1] == 0:\n",
    "                break\n",
    "\n",
    "        return b_all, cell_count_all, t_all, kn0_all, rewards_all, Q1_all, Q1_target_all, Q2_all, Q2_target_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make sure to comment out wandb before testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: ./Results12: File exists\n",
      "mkdir: ./Results12/Eval: File exists\n",
      "/var/folders/_2/b28clpbs6t36hp6grpxntpq40000gn/T/ipykernel_55670/1977211832.py:204: RuntimeWarning: divide by zero encountered in log10\n",
      "  eps = -np.log10(e/T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0\n",
      "Evaluation\n",
      "Episode:  1\n",
      "Episode:  2\n",
      "Episode:  3\n",
      "Episode:  4\n",
      "Episode:  5\n",
      "Episode:  6\n",
      "Episode:  7\n",
      "Episode:  8\n",
      "Episode:  9\n",
      "Episode:  10\n",
      "Evaluation\n",
      "Episode:  11\n",
      "Episode:  12\n",
      "Episode:  13\n",
      "Episode:  14\n",
      "Episode:  15\n",
      "Episode:  16\n",
      "Episode:  17\n",
      "Episode:  18\n",
      "Episode:  19\n",
      "Episode:  20\n",
      "Evaluation\n",
      "Episode:  21\n",
      "Episode:  22\n",
      "Episode:  23\n",
      "Episode:  24\n",
      "Episode:  25\n",
      "Episode:  26\n",
      "Episode:  27\n",
      "Episode:  28\n",
      "Episode:  29\n",
      "Episode:  30\n",
      "Evaluation\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "c = CDQL()\n",
    "# c.load_data(12)\n",
    "c.train(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
